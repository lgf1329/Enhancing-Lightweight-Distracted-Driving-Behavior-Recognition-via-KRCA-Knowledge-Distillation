# Enhancing-Lightweight-Distracted-Driving-Behavior-Recognition-via-KRCA-Knowledge-Distillation
ðŸš€ â€‹â€‹**KRCA-Distilled Lightweight Model for Real-time Distracted Driving Recognitionâ€‹â€‹**

â€‹â€‹**Revolutionizing edge deployment with multi-knowledge distillation and light robustness optimizationâ€‹â€‹**

This repository presents a groundbreaking lightweight framework for driver distraction recognition, achieving â€‹â€‹99.83% accuracyâ€‹â€‹ with only â€‹â€‹187.43K parametersâ€‹â€‹. Our novel KRCA (Knowledge Relation Comparison with Attention) distillation method synergistically combines four complementary knowledge types for the first time in driving behavior recognition, overcoming traditional distillation's limitations in handling light variations and behavioral similarities.

![FIG1](https://github.com/user-attachments/assets/7423dfef-c3dc-4f2f-b7e7-65a6da380ef4)

ðŸ”¬ â€‹â€‹**Core Innovation: Multi-Knowledge Distillation Frameworkâ€‹â€‹**

The KRCA distillation framework simultaneously optimizes:

1.**â€‹â€‹Hard labelsâ€‹â€‹** (L_cls) for final prediction alignment

2.â€‹â€‹**Soft labelsâ€‹â€‹** (L_TC + L_NC) with target/non-target class decomposition

3.**Sample relationsâ€‹â€‹** (L_RC) preserving cross-sample dependencies

4.â€‹â€‹**Inter-layer attentionâ€‹â€‹** (L_SC + L_AT) transferring spatial focus patterns

This integrated approach solves the critical problems of â€‹â€‹behavioral preference biasâ€‹â€‹ and â€‹â€‹feature mutual exclusionâ€‹â€‹ observed in single-knowledge distillation methods, particularly enhancing recognition of hand-occluded actions  in low-light conditions.

![FIG2](https://github.com/user-attachments/assets/a57d5f69-e033-46ef-9a46-9fc48697940b)
ðŸŒŸ â€‹â€‹**Key Performance Highlightsâ€‹**

Modified SqueezeNet_LW

â€¢Parameters reduced from 740.55K â†’ 187.43K (**74.7%â†“**)

â€¢FLOPs reduced from 750.32M â†’ 254.3M (**66.1%â†“**)

â€¢Inference speed: 120.88 FPS (**2.5Ã— faster than baseline**)
